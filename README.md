# ğŸš€ Dynamic Pricing Using Reinforcement Learning for Blinkit  


> ğŸ” **Author:** [Jasraj Shendge](https://www.linkedin.com/in/jasrajshendge)  
> ğŸ“ **Institution:** Manipal University Jaipur  
> ğŸ“… **Academic Year:** 2024â€“2025

---

## ğŸ“– Overview

This project implements a **Dynamic Pricing Strategy** for Blinkit using **Reinforcement Learning (RL)** and **Price Elasticity of Demand (PED)**. Unlike static pricing, dynamic pricing allows businesses to **adjust prices based on real-time demand, consumer behavior, and market conditions**.

We explore two RL algorithms â€” **Q-Learning** and **Deep Q-Network (DQN)** â€” and compare their effectiveness in optimizing pricing strategies.

---

## ğŸ¯ Objectives

- âœ… Predict and understand **Price Elasticity of Demand** (PED)  
- âœ… Implement **Q-Learning** and **DQN** for dynamic pricing  
- âœ… Maximize **revenue** and **profit margin** through smart decisions  
- âœ… Perform comparative analysis between RL techniques  
- âœ… Visualize and interpret the impact of price changes  

---

## ğŸ“Š Dataset Overview

The dataset is sourced from a grocery sales environment and contains the following key columns:

| Column | Description |
|--------|-------------|
| `Item_Identifier` | Unique product ID |
| `Item_Type` | Category of product |
| `Item_MRP` | Maximum Retail Price |
| `Item_Outlet_Sales` | Total sales generated |
| `Outlet_Location_Type` | Urban/Rural classification |
| `Outlet_Type` | Store type (Supermarket, Grocery, etc.) |

**Visualizations Included:**
- ğŸ“‰ Price vs Sales Scatter Plot  
- ğŸ“Š PED Distribution Histogram  
- ğŸ“ˆ RL Reward Convergence  
- ğŸ“Š Exploration vs Exploitation Trend  
- ğŸ“Š Revenue & Profit Comparison Charts  

---

## ğŸ§  Machine Learning Techniques

### ğŸ“Œ Step 1: PED Calculation using Regression

We evaluated **Random Forest**, **XGBoost**, and **Linear Regression** models. Based on RÂ² scores, **XGBoost** was selected for PED estimation.

- PED was calculated by simulating a Â±5% price change and recording the variation in predicted sales.
- Final PED scores were used to categorize items as elastic or inelastic.

### ğŸ“Œ Step 2: Reinforcement Learning Models

| Model | Description |
|-------|-------------|
| **Q-Learning** | Tabular model, easy to interpret |
| **Deep Q-Network (DQN)** | Neural network-based, supports large state-action spaces |

---

## âš™ï¸ Training Configurations

| Hyperparameter | Value |
|----------------|-------|
| Learning Rate (Î±) | 0.1 |
| Discount Factor (Î³) | 0.9 |
| Exploration Rate (Îµ) | Decays from 1.0 to 0.01 |
| Episodes | 1000 |

ğŸ“ˆ Visualizations track reward convergence, PED alignment, and exploration strategies.

---

## ğŸ§ª Experimental Results

| Item Type | Old Price | New Price (RL) | Revenue Change | Profit Margin Change |
|-----------|-----------|----------------|----------------|-----------------------|
| Dairy     | â‚¹50       | â‚¹55            | +8.5%          | +6.3%                |
| Snacks    | â‚¹30       | â‚¹27            | -3.2%          | +1.5%                |
| Beverages | â‚¹20       | â‚¹22            | +5.1%          | +4.8%                |

---

## ğŸ“Š Comparison: Q-Learning vs DQN

| Criteria | Q-Learning | DQN |
|----------|------------|-----|
| Interpretability | âœ… High | âš ï¸ Lower |
| Scalability | âŒ Low | âœ… High |
| Revenue Improvement | Moderate | High |
| Training Time | Fast | Slower |
| Use Case Suitability | Small to Mid-Size | Large-scale environments |

ğŸ“Œ *DQN emerges as the better approach for complex, high-dimensional pricing scenarios.*

---

## ğŸ“ Project Structure

ğŸ“‚ dynamic-pricing-rl
â”œâ”€â”€ main.ipynb # Q-Learning implementation
â”œâ”€â”€ dqn_model.ipynb # DQN agent implementation
â”œâ”€â”€ evaluation.ipynb # Result analysis & visualization
â”œâ”€â”€ RL_Pricing_Comparison.csv # Q-Learning vs. Static Pricing
â”œâ”€â”€ PED_Results.csv # PED values by item
â”œâ”€â”€ q_table.npy # Trained Q-table
â”œâ”€â”€ model_weights.h5 # DQN model weights
â””â”€â”€ README.md # Project overview (this file)

## âš¡ How to Run

### 1ï¸âƒ£ Setup

bash
git clone https://github.com/jasrajshendge/dynamic-pricing-rl.git
cd dynamic-pricing-rl
pip install -r requirements.txt

2ï¸âƒ£ Run Q-Learning Agent
bash
Copy
Edit
jupyter notebook main.ipynb
3ï¸âƒ£ Run DQN Agent
bash
Copy
Edit
jupyter notebook dqn_model.ipynb

### Conclusion
This project proves the effectiveness of Reinforcement Learning in dynamic pricing when fused with Price Elasticity of Demand. The Q-Learning model offered strong baseline performance, while the DQN agent outperformed in real-world scalability and revenue lift. Together, these models showcase a powerful approach to automated, demand-sensitive pricing in the e-commerce domain.

ğŸ”® Future Scope
Integrate competitor pricing and seasonal trends

Use Multi-Agent Reinforcement Learning (MARL) for multi-brand scenarios

Develop a real-time pricing engine for deployment

Explore hierarchical RL for region-based or product-cluster pricing

ğŸ“¬ Connect with Me
ğŸ‘¨â€ğŸ’» Jasraj Shendge
ğŸ“§ jasraj.shendge@example.com
ğŸ”— LinkedIn
ğŸ“‚ GitHub

vbnet
Copy
Edit

Let me know if you'd like a `.pdf` version or need help designing a GitHub repo banner or README visu
